# ğŸ§© ConcurML  
### âš¡ Performance Intelligence for Concurrent AI Systems  

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" />
  <img src="https://img.shields.io/badge/Framework-Streamlit-red.svg" />
  <img src="https://img.shields.io/badge/License-MIT-green.svg" />
  <img src="https://img.shields.io/badge/Status-Active-success.svg" />
  <img src="https://img.shields.io/badge/AI-LLM%20Powered-purple.svg" />
</p>

<p align="center">
  <b>Monitor. Diagnose. Optimize.</b><br/>
  Built for modern AI workloads running at scale.
</p>


---

## ğŸš€ What is ConcurML?

ConcurML is a real-time performance intelligence platform designed to profile and optimize **concurrent AI/ML workloads**.

It bridges the gap between:
AI Model Execution â†’ Hardware Utilization â†’ Bottleneck Detection â†’ Optimization Proof


Unlike traditional system monitors, ConcurML doesnâ€™t just show numbers â€”  
it explains performance behavior and validates measurable improvements.

---

### ğŸ”§ Requirements

- Python 3.10+
- Ollama installed locally (with llama3)
- NVIDIA GPU (optional, for GPU telemetry)
- Streamlit

---

## âš ï¸ The Challenge

Modern AI deployments face:

- CPU saturation during concurrent inference  
- GPU underutilization or memory overflow  
- Unpredictable latency under load  
- Lack of optimization validation  

Existing tools show metrics.  
They donâ€™t connect them to AI execution logic.

---

## ğŸ’¡ The ConcurML Approach

âœ” Run multiple models concurrently  
âœ” Capture live hardware telemetry  
âœ” Detect bottlenecks automatically  
âœ” Benchmark optimized vs naive execution  
âœ” Generate structured AI-driven diagnostics  

From raw telemetry to actionable intelligence.

---


## ğŸ”´ bad_model.py

# The naive implementation:
-Uses nested Python loops
-Runs purely in interpreted Python
-Creates high interpreter overhead
-Fails to leverage BLAS or SIMD optimizations
-Causes CPU core saturation

# Impact:
-High execution time
-Low throughput (tokens/sec)
-CPU bottleneck due to software inefficiency

## ğŸŸ¢ optimized_model.py

# The optimized implementation:
-Uses NumPy vectorization (C-backed BLAS execution)
-Reduces Python-level looping
-Improves memory locality and cache usage
-Utilizes multi-core CPU instructions efficiently

# Impact:
-Significantly reduced execution time
-Higher throughput
-Lower CPU saturation
-Improved scalability under concurrency

---

## âœ¨ Key Capabilities

- ğŸš€ Parallel multi-model execution engine  
- ğŸ“¡ Real-time CPU, GPU, VRAM & I/O tracking  
- ğŸ›¡ Predictive resource estimation  
- ğŸ“Š Interactive performance dashboard  
- ğŸ† Optimization benchmarking framework  
- ğŸ¤– Automated bottleneck classification using LLMs  

---

## ğŸ— System Architecture


User Interface (Streamlit)
â†“
Concurrent Execution Engine
â†“
Model Layer (LLMs + Synthetic Models)
â†“
Telemetry Layer (CPU / GPU Monitoring)
â†“
Analytics Engine
â†“
AI Diagnostic Report


---

## ğŸ›  Technology Stack

| Layer | Technology |
|-------|------------|
| Compute | AMD Ryzen Processor |
| Backend | Python |
| UI | Streamlit |
| AI Engine | Ollama (LLMs) |
| Telemetry | psutil + NVIDIA NVML |
| Analytics | Pandas |

---

## âš¡ Quick Start

```bash
git clone https://github.com/gorainakash/ConcurML.git
cd Hackathon_AMD
pip install -r requirements.txt
streamlit run MAIN.py
